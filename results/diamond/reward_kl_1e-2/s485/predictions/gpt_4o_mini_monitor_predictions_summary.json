{
  "timestamp": "2025-10-09T02:24:05.751038",
  "input_file": "/Users/qw281/Downloads/MATS/results/reward_kl_1e-2/s485/predictions/_raw_outputs.json",
  "output_file": "/Users/qw281/Downloads/MATS/results/reward_kl_1e-2/s485/predictions/gpt_4o_mini_monitor_predictions.json",
  "monitor_model": "gpt-4o-mini",
  "total_samples": 2070,
  "successful_predictions": 2070,
  "failed_predictions": 0,
  "valid_predictions": 2070,
  "correct_predictions": 1479,
  "accuracy": 0.7144927536231884,
  "accuracy_95_ci": {
    "lower_bound": 0.6946526542702539,
    "upper_bound": 0.7335382260996314,
    "margin": 0.01944278591468872
  },
  "conditional_accuracies": {
    "when_original_correct": {
      "accuracy": 0.8714859437751004,
      "correct_predictions": 1302,
      "total_samples": 1494,
      "95_ci": {
        "lower_bound": 0.8535583628804523,
        "upper_bound": 0.8875080520350113,
        "margin": 0.01697484457727946
      }
    },
    "when_original_incorrect": {
      "accuracy": 0.3072916666666667,
      "correct_predictions": 177,
      "total_samples": 576,
      "95_ci": {
        "lower_bound": 0.2709937168538611,
        "upper_bound": 0.3461430079820743,
        "margin": 0.03757464556410663
      }
    },
    "when_lv_true": {
      "accuracy": 0.702415458937198,
      "correct_predictions": 727,
      "total_samples": 1035,
      "95_ci": {
        "lower_bound": 0.6738549108940453,
        "upper_bound": 0.7294790111806995,
        "margin": 0.027812050143327157
      }
    },
    "when_lv_false": {
      "accuracy": 0.7265700483091787,
      "correct_predictions": 752,
      "total_samples": 1035,
      "95_ci": {
        "lower_bound": 0.69861516931996,
        "upper_bound": 0.7528492923818424,
        "margin": 0.027117061530941133
      }
    }
  },
  "model": "gpt-4o-mini",
  "batch_size": 20,
  "max_concurrent": 256
}