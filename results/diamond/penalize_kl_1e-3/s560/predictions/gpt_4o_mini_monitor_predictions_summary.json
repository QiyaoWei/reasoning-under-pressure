{
  "timestamp": "2025-10-09T03:05:27.374384",
  "input_file": "/Users/qw281/Downloads/MATS/results/penalize_kl_1e-3/s560/predictions/_raw_outputs.json",
  "output_file": "/Users/qw281/Downloads/MATS/results/penalize_kl_1e-3/s560/predictions/gpt_4o_mini_monitor_predictions.json",
  "monitor_model": "gpt-4o-mini",
  "total_samples": 2070,
  "successful_predictions": 2070,
  "failed_predictions": 0,
  "valid_predictions": 2070,
  "correct_predictions": 1623,
  "accuracy": 0.7840579710144927,
  "accuracy_95_ci": {
    "lower_bound": 0.7658146191373001,
    "upper_bound": 0.8012489791903856,
    "margin": 0.01771718002654272
  },
  "conditional_accuracies": {
    "when_original_correct": {
      "accuracy": 0.893491124260355,
      "correct_predictions": 1510,
      "total_samples": 1690,
      "95_ci": {
        "lower_bound": 0.8778807024355403,
        "upper_bound": 0.9073167510061698,
        "margin": 0.014718024285314794
      }
    },
    "when_original_incorrect": {
      "accuracy": 0.29736842105263156,
      "correct_predictions": 113,
      "total_samples": 380,
      "95_ci": {
        "lower_bound": 0.2536232731959661,
        "upper_bound": 0.3451694146154388,
        "margin": 0.04577307070973637
      }
    },
    "when_lv_true": {
      "accuracy": 0.8357487922705314,
      "correct_predictions": 865,
      "total_samples": 1035,
      "95_ci": {
        "lower_bound": 0.8119428387097343,
        "upper_bound": 0.8570716621063699,
        "margin": 0.02256441169831779
      }
    },
    "when_lv_false": {
      "accuracy": 0.7323671497584541,
      "correct_predictions": 758,
      "total_samples": 1035,
      "95_ci": {
        "lower_bound": 0.7045721700843719,
        "upper_bound": 0.7584436211279244,
        "margin": 0.0269357255217763
      }
    }
  },
  "model": "gpt-4o-mini",
  "batch_size": 20,
  "max_concurrent": 256
}