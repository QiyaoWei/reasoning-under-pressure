{
  "timestamp": "2025-10-11T12:57:28.971568",
  "input_file": "/Users/qw281/Downloads/MATS/results/reward_verbosity_1e-9/s515/predictions/_raw_outputs.json",
  "output_file": "/Users/qw281/Downloads/MATS/results/reward_verbosity_1e-9/s515/predictions/gpt_4o_mini_monitor_predictions.json",
  "monitor_model": "gpt-4o-mini",
  "total_samples": 2070,
  "successful_predictions": 2070,
  "failed_predictions": 0,
  "valid_predictions": 2070,
  "correct_predictions": 1561,
  "accuracy": 0.7541062801932367,
  "accuracy_95_ci": {
    "lower_bound": 0.7350964171765386,
    "upper_bound": 0.7721747609193164,
    "margin": 0.018539171871388923
  },
  "conditional_accuracies": {
    "when_original_correct": {
      "accuracy": 0.8840222085132634,
      "correct_predictions": 1433,
      "total_samples": 1621,
      "95_ci": {
        "lower_bound": 0.8675188197462379,
        "upper_bound": 0.8987097825837506,
        "margin": 0.015595481418756364
      }
    },
    "when_original_incorrect": {
      "accuracy": 0.28507795100222716,
      "correct_predictions": 128,
      "total_samples": 449,
      "95_ci": {
        "lower_bound": 0.24528102467932095,
        "upper_bound": 0.32852124929396437,
        "margin": 0.0416201123073217
      }
    },
    "when_lv_true": {
      "accuracy": 0.8154589371980676,
      "correct_predictions": 844,
      "total_samples": 1035,
      "95_ci": {
        "lower_bound": 0.790673975559003,
        "upper_bound": 0.8379108719703724,
        "margin": 0.02361844820568471
      }
    },
    "when_lv_false": {
      "accuracy": 0.6927536231884058,
      "correct_predictions": 717,
      "total_samples": 1035,
      "95_ci": {
        "lower_bound": 0.6639770604883244,
        "upper_bound": 0.7201046457355973,
        "margin": 0.028063792623636483
      }
    }
  },
  "model": "gpt-4o-mini",
  "batch_size": 20,
  "max_concurrent": 256
}