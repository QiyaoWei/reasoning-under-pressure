{
  "timestamp": "2025-10-11T12:57:23.013423",
  "input_file": "/Users/qw281/Downloads/MATS/results/reward_verbosity_1e-9/s545/predictions/_raw_outputs.json",
  "output_file": "/Users/qw281/Downloads/MATS/results/reward_verbosity_1e-9/s545/predictions/gpt_4o_mini_monitor_predictions.json",
  "monitor_model": "gpt-4o-mini",
  "total_samples": 2070,
  "successful_predictions": 2070,
  "failed_predictions": 0,
  "valid_predictions": 2070,
  "correct_predictions": 1557,
  "accuracy": 0.7521739130434782,
  "accuracy_95_ci": {
    "lower_bound": 0.7331189096374612,
    "upper_bound": 0.7702946929595661,
    "margin": 0.01858789166105246
  },
  "conditional_accuracies": {
    "when_original_correct": {
      "accuracy": 0.8776266996291718,
      "correct_predictions": 1420,
      "total_samples": 1618,
      "95_ci": {
        "lower_bound": 0.8607578818444221,
        "upper_bound": 0.8927066404452999,
        "margin": 0.015974379300438893
      }
    },
    "when_original_incorrect": {
      "accuracy": 0.3030973451327434,
      "correct_predictions": 137,
      "total_samples": 452,
      "95_ci": {
        "lower_bound": 0.2625332090210001,
        "upper_bound": 0.34698015016203,
        "margin": 0.042223470570514926
      }
    },
    "when_lv_true": {
      "accuracy": 0.7806763285024154,
      "correct_predictions": 808,
      "total_samples": 1035,
      "95_ci": {
        "lower_bound": 0.754454659101838,
        "upper_bound": 0.804822211364574,
        "margin": 0.025183776131368015
      }
    },
    "when_lv_false": {
      "accuracy": 0.7236714975845411,
      "correct_predictions": 749,
      "total_samples": 1035,
      "95_ci": {
        "lower_bound": 0.6956388287117675,
        "upper_bound": 0.7500499682347882,
        "margin": 0.027205569761510392
      }
    }
  },
  "model": "gpt-4o-mini",
  "batch_size": 20,
  "max_concurrent": 256
}