{
  "timestamp": "2025-10-11T12:18:44.855029",
  "input_file": "/Users/qw281/Downloads/MATS/results/penalize_verbosity_1e-7/s880/predictions/_raw_outputs.json",
  "output_file": "/Users/qw281/Downloads/MATS/results/penalize_verbosity_1e-7/s880/predictions/gpt_4o_monitor_predictions.json",
  "monitor_model": "gpt-4o",
  "total_samples": 2070,
  "successful_predictions": 2070,
  "failed_predictions": 0,
  "valid_predictions": 2063,
  "correct_predictions": 1429,
  "accuracy": 0.6926805622879302,
  "accuracy_95_ci": {
    "lower_bound": 0.6724282472856234,
    "upper_bound": 0.7122166400167699,
    "margin": 0.019894196365573245
  },
  "conditional_accuracies": {
    "when_original_correct": {
      "accuracy": 0.7318668252080857,
      "correct_predictions": 1231,
      "total_samples": 1682,
      "95_ci": {
        "lower_bound": 0.710185743663454,
        "upper_bound": 0.7524912156193233,
        "margin": 0.021152735977934663
      }
    },
    "when_original_incorrect": {
      "accuracy": 0.5196850393700787,
      "correct_predictions": 198,
      "total_samples": 381,
      "95_ci": {
        "lower_bound": 0.46957209444333037,
        "upper_bound": 0.5694049950988564,
        "margin": 0.04991645032776304
      }
    },
    "when_lv_true": {
      "accuracy": 0.5242718446601942,
      "correct_predictions": 540,
      "total_samples": 1030,
      "95_ci": {
        "lower_bound": 0.4937391290258274,
        "upper_bound": 0.5546241858455215,
        "margin": 0.03044252840984706
      }
    },
    "when_lv_false": {
      "accuracy": 0.8606001936108422,
      "correct_predictions": 889,
      "total_samples": 1033,
      "95_ci": {
        "lower_bound": 0.8381393159347142,
        "upper_bound": 0.8803890508296363,
        "margin": 0.02112486744746103
      }
    }
  },
  "model": "gpt-4o",
  "batch_size": 20,
  "max_concurrent": 256
}