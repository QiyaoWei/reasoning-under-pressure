{
  "timestamp": "2025-10-11T12:34:48.496734",
  "input_file": "/Users/qw281/Downloads/MATS/results/penalize_verbosity_1e-7/s680/predictions/_raw_outputs.json",
  "output_file": "/Users/qw281/Downloads/MATS/results/penalize_verbosity_1e-7/s680/predictions/gpt_4o_mini_monitor_predictions.json",
  "monitor_model": "gpt-4o-mini",
  "total_samples": 2070,
  "successful_predictions": 2070,
  "failed_predictions": 0,
  "valid_predictions": 2070,
  "correct_predictions": 1564,
  "accuracy": 0.7555555555555555,
  "accuracy_95_ci": {
    "lower_bound": 0.7365798768882248,
    "upper_bound": 0.7735844828317511,
    "margin": 0.018502302971763133
  },
  "conditional_accuracies": {
    "when_original_correct": {
      "accuracy": 0.872479240806643,
      "correct_predictions": 1471,
      "total_samples": 1686,
      "95_ci": {
        "lower_bound": 0.8557064565983419,
        "upper_bound": 0.8875585363652803,
        "margin": 0.015926039883469165
      }
    },
    "when_original_incorrect": {
      "accuracy": 0.2421875,
      "correct_predictions": 93,
      "total_samples": 384,
      "95_ci": {
        "lower_bound": 0.20202853452750144,
        "upper_bound": 0.2874535837533632,
        "margin": 0.04271252461293088
      }
    },
    "when_lv_true": {
      "accuracy": 0.8318840579710145,
      "correct_predictions": 861,
      "total_samples": 1035,
      "95_ci": {
        "lower_bound": 0.8078827294106846,
        "upper_bound": 0.8534308850650901,
        "margin": 0.02277407782720272
      }
    },
    "when_lv_false": {
      "accuracy": 0.6792270531400966,
      "correct_predictions": 703,
      "total_samples": 1035,
      "95_ci": {
        "lower_bound": 0.6501721598259925,
        "upper_bound": 0.7069564442067765,
        "margin": 0.028392142190392003
      }
    }
  },
  "model": "gpt-4o-mini",
  "batch_size": 20,
  "max_concurrent": 256
}