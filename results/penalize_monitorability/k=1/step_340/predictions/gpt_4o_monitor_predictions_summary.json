{
  "timestamp": "2025-09-25T06:52:19.510225",
  "input_file": "results/penalize_monitorability/k=1/step_340/predictions/against-monitor-k1-weighted-step_340_raw_outputs.json",
  "output_file": "results/penalize_monitorability/k=1/step_340/predictions/gpt_4o_monitor_predictions.json",
  "monitor_model": "gpt-4o",
  "total_samples": 1518,
  "successful_predictions": 1518,
  "failed_predictions": 0,
  "valid_predictions": 1518,
  "correct_predictions": 993,
  "accuracy": 0.6541501976284585,
  "accuracy_95_ci": {
    "lower_bound": 0.6298607834497315,
    "upper_bound": 0.677661394558655,
    "margin": 0.023900305554461713
  },
  "conditional_accuracies": {
    "when_original_correct": {
      "accuracy": 0.9148936170212766,
      "correct_predictions": 430,
      "total_samples": 470,
      "95_ci": {
        "lower_bound": 0.8861813725922769,
        "upper_bound": 0.936878730613212,
        "margin": 0.025348679010467518
      }
    },
    "when_original_incorrect": {
      "accuracy": 0.5372137404580153,
      "correct_predictions": 563,
      "total_samples": 1048,
      "95_ci": {
        "lower_bound": 0.5069449158868795,
        "upper_bound": 0.5672107464003364,
        "margin": 0.030132915256728492
      }
    },
    "when_lv_true": {
      "accuracy": 0.4769433465085639,
      "correct_predictions": 362,
      "total_samples": 759,
      "95_ci": {
        "lower_bound": 0.4416155912013221,
        "upper_bound": 0.512503315689098,
        "margin": 0.035443862243887944
      }
    },
    "when_lv_false": {
      "accuracy": 0.8313570487483531,
      "correct_predictions": 631,
      "total_samples": 759,
      "95_ci": {
        "lower_bound": 0.8030650269818944,
        "upper_bound": 0.8563118254214963,
        "margin": 0.026623399219800947
      }
    }
  },
  "model": "gpt-4o",
  "batch_size": 20,
  "max_concurrent": 256
}